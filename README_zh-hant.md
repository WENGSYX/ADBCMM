<h3 align="center">
    <big><big><strong>ADBCMM</strong></big></big>  :Acronym Disambiguation
</h3>
<h3 align="center">
by Building Counterfactuals and Multilingual Mixing
</h3>
<h4 align="center">
    通過多語言中混合訓練和構建反事實樣本提升縮略詞消歧效能
</h4>
<hr>

<h4 align="center">
    <p>
        <a href="https://github.com/WENGSYX/ADBCMM/blob/master/README_en.md">English</a> |
        <a href="https://github.com/WENGSYX/ADBCMM/blob/master/README.md">简体中文</a> |
        <b>繁體中文</b>
    <p>
</h4>

<h3 align="center">
    <p>SDU@AAAI2022 縮略詞消歧任務冠軍方案</p></h3>



####                        由於世界各國的科學文獻中都包含了大量的首字母縮略詞，但這些縮略詞會阻礙研究人員閱讀科學文獻。 囙此[ SDU@AAAI2022 ]（ https://sites.google.com/view/sdu-aaai22/ ）提出了[縮略詞消歧任務（AD）]（ https://competitions.codalab.org/competitions/34899 ）。 AD任務給定一篇從科學文獻中截取的短文，並給定一份含多個縮略語—長句的字典，字典中每個對應的縮略語均含有2-14條長句，要求針對不同的短語，選出其中最合適的長句。

####               這個比賽給了四個不同語言或領域的數据集，分別是英語（法律），英語（科學），法語和西班牙語。 對於英語數据集，本文不作對比（但使用我們的ADBCMM方法，mdeberta-base模型的效果在英語數据集上仍然能超過deberta-xxlarge模型的baseline效能），本文重點放在低資源數據集中

<center><img src="img/1.png" alt="img" style="zoom:50%;" /></center



####        在本存儲庫中，保存著我們在 SDU@AAAI2022 任務中冠軍方案程式碼，您可基於本程式碼複現我們的成績，或者可閱讀我們的論文，瞭解具體資訊。



## ADBCMM

##### 由於低資源數据集無論是預訓練階段還是微調階段，相比起英語數据集，都更加難以學習到相關的資訊，模型很容易造成偏見，更加偏向於選擇訓練集中存在的樣本。 囙此我們選擇認為想要獲得更好的泛化效能，就需要額外的數據進行增强。 但這些數據從哪來呢？ 總不能人工標注吧？

##### 囙此，我們做了一個假設，像是法語和西班牙語這種數据集，在幾千年前應該是源於同一種語言，這些語言在語法和結構上，應當存在著一些深層次的關係。 囙此能否讓模型從不同語言的數據集中，都學到縮略詞消歧的深層含義呢？

##### 多國語言預訓練模型為我們提供了可能，畢竟多國語言預訓練模型本身，就是在不同語言的語料庫中進行訓練的。 顯然，這個模型預訓練時成功學會到不同語言之間深層次的關係。 我們對這個模型也同樣在下游任務中使用多國語言數据集進行微調，這是一個十分簡單的想法。

##### 具體來說，為了提升低資源數据集的效能，我們基於課程學習的方法，在預訓練模型的基礎上，我們首先對四個不同語言的數据集混合訓練，之後再在相關的數据集上”微微調 “。實驗發現，這種方法，最多能比純粹使用單語種數據微調，效果提升接近15%！這是十分驚人的。

<center><img src="img/3.png" alt="img" style="zoom:50%;" /></center>







## 其餘方法



#### 模型框架

##### 我們使用了多項選擇模型，對於每一條短文，我們逐一將“長句-[SEP]-縮略詞-[SEP]-短文“作為模型的輸出，讓模型選擇最有可能的一條。為了更加並行化得訓練模型，如果長句較少，我們使用“padding” 進行填充，使每個batch，均在14條該短文樣本中進行選擇。

<center><img src="img/2.png" alt="img" style="zoom:50%;" /></center>

##### 注意：上下文所稱Baseline，均指將預訓練模型放入此框架中，而不使用本文所提及的其他方法。



#### 對抗學習與D-Drop

##### 眾所周知，在NLP領域，使用對抗學習或D-Drop，能有效新增模型的魯棒性，帶來明顯的效果提升。我們也嘗試使用了這兩種方法，並為我們的模型帶來1-5%的提升。



#### Child-Tuning

##### 這是一種比較新穎的方法，在訓練過程中，只微調小部分的權重 （對於小部分權重的選擇，有兩種做法：Child-Tuning-D是任務無關做法，直接在伯努利分佈中選擇需要微調的權重；Child-Tuning-F是任務相關做法，在第一輪中使用全參數微調模型，之後會進行蒐索，查找整個微調過程中，變化最大的那一批權重，在後面幾輪的訓練中， 都只會對變化大的那一部分的權重進行微調，其餘的保持不變。一般來說，Child-Tuning-F的效果好一些，但是時間會比較久，建議按需選擇）。



#### 預訓練模型

##### 我們使用了 [MDeberta-v3-base模型](https://huggingface.co/microsoft/mdeberta-v3-base)，這個模型在包含一百種語言的CC100數據集中進行了訓練，應該算得上是現時最好的多語言預訓練Encoder模型。



## 實驗結果



<center><img src="img/4.png" alt="img" style="zoom:50%;" /></center>

##### 這是消融實驗與對比實驗的表格，頭三欄是三個模型的baseline效果，BETO是西班牙語預訓練模型，Flaubert-base-cased是法語預訓練模型。mDeberta-v3-base我們也是在單語種中做了對比實驗。由錶可見，mDeberta-v3-base的如果論單語種微調的效能，其實還是遠不如只在 單個語種中進行預訓練的另外兩個模型。

##### 不過，如果加上我們的ADBCMM，也就是使用四份數据集，先進行訓練，之後再在單語種中訓練，這能大幅提升模型的效果。其中，”ALLs“表示單模型，使用所有方法在dev集中達到的最佳成績。”Finally in Test “，使用了多個模型進行融合，其中包括五折融合/隨機融合/加權融合在內的諸多融合策略，達到了最佳的效果。



## 最終排名

##### 雖然在本文開頭就已經強調了一遍，但還是想再具體展示下：

<center><img src="img/5.png" alt="img" style="zoom:50%;" /></center>

#### 整整比第二名領先4-6%的F1，充分證明了我們的方法具有優越性與領先性。



## 複現程式碼

#### 最後更新日期：21年12月8日剩餘內容稍待片刻
